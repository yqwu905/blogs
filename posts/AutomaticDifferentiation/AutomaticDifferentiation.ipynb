{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 自动微分(Automatic Differentiation)\n",
        "description: '求任意函数的微分是数值方法中很重要的一个内容, 不同于计算物理中常用的简单数值微分, 这里我们介绍在ML等领域中常用的另一种微分方式: 自动微分'\n",
        "author: Yuanqing Wu\n",
        "date: 08/26/2021\n",
        "image: 'https://image.yqwu.site/hz27.jpg'\n",
        "---"
      ],
      "id": "74281fe2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 自动微分(Automatic Differentiation)\n",
        "## 微分算法\n",
        "对于几乎所有最优化方法来说, 最基础的一步就是求任意函数的微分. 例如梯度下降法(Gradient Descent). 因此, 微分算法的重要性自然不言而喻. 目前, 一共存在四种微分算法:\n",
        "* 手动微分(Manual Differentiation), 即手动求出函数的微分, 并直接编码进代码里.\n",
        "* 数值微分(Numerical Differentiation). 顾名思义, 求解任意函数在某一点的微分的数值解.\n",
        "* 符号微分(Symbolic Differentiation). 即Mathematica中提供的微分, 给出任意函数微分的解析表达式.\n",
        "* 自动微分(Automatic Differentiation).\n",
        "\n",
        "其中手动微分由于每次修改模型, 我们都需要更改求解梯度的代码, 并且对于维度较高的函数编码量很大, 因此较少采用, 我们在此也不会涉及.\n",
        "\n",
        "### 数值微分(Numerical Differentiation)\n",
        "数值微分是最简单的一种计算微分的方法, 原理就是基于微分的定义:\n",
        "$$ f'(x) = \\lim_{\\epsilon \\to 0} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon}$$\n",
        "我们将$\\epsilon$ 取一个极小值(例如$0.00001$), 代入计算即可得到数值微分. 较为常用的是中心差分法:\n",
        " $$ f'(x) = \\lim_{\\epsilon \\to 0} \\frac{f(x+\\epsilon) - f(x-\\epsilon)}{2 \\epsilon} $$\n",
        "数值微分的优点是**编码非常容易**, 但缺点也很明显:**第一, 计算量很大, 计算梯度时对于每个变量都需要计算两次函数值; 第二, 误差较大, 这一点在计算物理/数值方法课上应该都有过详细的讨论**.\n",
        "\n",
        "### 符号微分(Symbolic Differentiation)\n",
        "符号微分即求得函数的微分的解析表达式. 其优点在于得到**是精确解, 并且只需要求解一次表达式, 函数各点的微分都可以得到**, 而缺点也很明显, **第一, 随着函数表达式复杂度的上升, 得到的微分解析式复杂度可能极速膨胀, 即所谓的\"表达式膨胀\"(expression swell)问题; 第二, 符号微分只能求解具有数学表达式形式的函数, 但很多时候我们使用的函数并不具有一个显式的表达式形式; 第三, 符号微分难以求解存在不可微点的函数.**\n",
        "\n",
        "### 自动微分(Automatic Differentiation)\n",
        "终于到了今天的主角, 自动微分. 自动微分的想法其实可以看作手动微分和数值微分的结合: 由于绝大多数函数的微分都可以通过查表得到, 因此我们只需要实现对几个基本函数的手动微分, 之后利用链式法则, 每步都带入数值求解, 就能实现对于任意函数的精确微分了.  \n",
        "自动微分是目前综合性能最好的微分算法, 同时具有精确求解, 速度快, 可求解含有逻辑控制语句与部分不可微的函数等优点.\n",
        "\n",
        "## 自动微分-Forward Mode\n",
        "我们从一个例子开始讲解前向的自动微分算法.  \n",
        "考虑函数:\n",
        "$$ f(x_1, x_2) = \\ln (x_1) + x_1 x_2 -\\sin(x_2) $$\n",
        "可以将其转化为如下的AST形式\n",
        "![2021-08-17-01-20-17](https://image.yqwu.site/2021-08-17-01-20-17.png)\n",
        "各节点分别为:\n",
        "* $v_{-1}$ :$x_1$ \n",
        "* $v_0$ :$x_2$\n",
        "* $v_1$ :$\\ln v_{-1}$\n",
        "* $v_2$ :$v_{-1}\\times v_0$\n",
        "* $v_3$ :$\\sin v_0$\n",
        "* $v_4$ :$v_1 + v_2$\n",
        "* $v_5$ :$v_4+v_3$  \n",
        "之后, 就可以很轻松的求解函数在各步的函数值与导数值, 如下表所示.\n",
        "![2021-08-17-01-20-57](https://image.yqwu.site/2021-08-17-01-20-57.png)\n",
        "实现前向自动微分最简单的方法就是*Dual Number*法. 其核心思想是用$x+\\epsilon$替换 $x$作为输入, 利用Taylor展开:\n",
        " $$ f(x+b \\epsilon) = f(x) + f'(x)b \\epsilon $$\n",
        " 例如, 我们需要求解$\\frac{\\partial f}{\\partial x_1}$, 则将$v_{-1}$ 从$x_1$改为$x_1 + \\epsilon$. 之后, 我们需要实现各种基本函数下$\\epsilon$ 的作用率, 例如:\n",
        " * $(a+b \\epsilon) + (c+ d \\epsilon) = (a+c) + (b+d) \\epsilon$\n",
        " * $(a+b \\epsilon)\\times (c+d \\epsilon) = ac + (bc+ ad)\\epsilon$\n",
        " *  $\\ln (a+b \\epsilon) = \\ln a + \\frac{b}{a}\\epsilon$\n",
        " * $\\sin (a+b \\epsilon) = \\sin(a)+ \\cos (a) b \\epsilon$  \n",
        "之后, 依次计算各个节点:\n",
        "* $v_{-1} = 2 + \\epsilon$\n",
        "* $v_0 = 5$\n",
        "* $v_1 = \\ln 2 + \\frac{\\epsilon}{2}$ \n",
        "* $v_2 = 10 + 5 \\epsilon$\n",
        "* $v_3 = \\sin 5$\n",
        "* $v_4 = 10 + \\ln 2 + \\frac{11 \\epsilon}{2} \\approx 10.693 + 5.5 \\epsilon$\n",
        "* $v_5 = 10.693 + \\sin 5 + 5.5 \\epsilon \\approx 11.652 + 5.5 \\epsilon$  \n",
        "\n",
        "由此, 我们得到了在$(x_1=2, x_2=5)$处, 函数值为$10.693$, 对$x_1$的导数值为$5.5$.  \n",
        "代码上的实现也是很简单的, 我们只要构造Dual Number类型:"
      ],
      "id": "3e39c5bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "struct DualNumber <: Number\n",
        "\ta::Float64;\n",
        "\tϵ::Float64;\n",
        "end"
      ],
      "id": "857eda44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "然后将各个基本函数重载(这里我们只重载了几个需要的函数):"
      ],
      "id": "6b8fddd5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 重载运算符, 实现对DualNumber的运算\n",
        "Base.:+(x::DualNumber, y::DualNumber) = DualNumber(x.a + y.a, x.ϵ + y.ϵ);\n",
        "Base.:-(x::DualNumber, y::DualNumber) = DualNumber(x.a - y.a, x.ϵ - y.ϵ);\n",
        "Base.:*(x::DualNumber, y::DualNumber) = DualNumber(x.a * y.a, x.ϵ * y.a + x.a * y.ϵ);\n",
        "Base.log(x::DualNumber) = DualNumber(log(x.a), x.ϵ/x.a);\n",
        "Base.sin(x::DualNumber) = DualNumber(sin(x.a), cos(x.a) * x.ϵ);\n",
        "Base.tan(x::DualNumber) = DualNumber(tan(x.a), sec(x.a)^2*x.ϵ);\n",
        "Base.sqrt(x::DualNumber) = DualNumber(sqrt(x.a), x.ϵ/(2*sqrt(x.a)));"
      ],
      "id": "7944df66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "实现前向自动微分的过程之后就相当简单了:"
      ],
      "id": "acc0f2bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function FowardAD(f, v::Vector{Float64})::Vector{Float64}\n",
        "\tgrad = Float64[]\n",
        "\tfor i in 1:length(v)\n",
        "\t\tdualV = map(x->x==i ? DualNumber(v[x], 1) : DualNumber(v[x], 0), 1:length(v));\n",
        "\t\tpush!(grad, f(dualV).ϵ)\n",
        "\tend\n",
        "\tgrad\n",
        "end"
      ],
      "id": "667b63dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "你很快就能发现我们这么做的优点:**我们不需要对代码做任何更改, 我们可以接受并求解任何一个你能定义的函数**. `FowardAD`函数接受一个任意的函数$f$, 与一个$n$维的向量$[v_1,  v_2,\\cdots,v_{n}]$, 返回$f$在 $v$处的梯度 $[\\partial f/\\partial v_i]$.  \n",
        "下面是两个例子  "
      ],
      "id": "0bce8458"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "g(x) = log(x[1]) + x[1]*x[2] - sin(x[2])\n",
        "f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x);\n",
        "y = [2., 5.]\n",
        "x = [0.986403, 0.140913, 0.294963, 0.837125, 0.650451];\n",
        "println(FowardAD(f, x));\n",
        "println(FowardAD(g, y));"
      ],
      "id": "2c3e7e8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "可以看出, 对于$f(x)$ 这种维数不定, 难以写出解析形式的函数, 代数微分难以求解, 但自动微分依然能得出相当精确的结果.\n",
        "\n",
        "## 自动微分-Backward Mode\n",
        "你可能会疑惑, 我们有了前向自动微分, 为什么还要研究什么Backward Mode自动微分呢? 答案很简单, 从前面的介绍你应当注意到, 对于一个$n$ 维的函数, 如果我们想要求得它在某点的梯度, 那么我们需要进行$n$ 次前向自动微分. 对于目前很多应用, 例如深度学习等, 函数的输入维数都是非常巨大的, 在这种情况下前向自动微分的效率就非常低, 这时就需要反向自动微分.  \n",
        "反向自动微分的原理也非常简单, 先进行一次正向的求解, 计算出每个节点处的函数值, 之后利用链式法则, 从后向前依次求出各节点的导数. 仍然用之前的例子:\n",
        "$$ f(x_1, x_2) = \\ln x_1 + x_1\\times x_2 - \\sin x_2$$\n",
        "![2021-08-17-01-22-35](https://image.yqwu.site/2021-08-17-01-22-35.png)\n",
        "其中$\\overline{v_{i}}$ 指代$\\partial y/\\partial v_{i}$. 需要注意的是, $v_{i}$ 应该被保存, 以供后续使用.  \n",
        "反向自动微分的原理非常简单, 但是实现上较前向自动微分困难一些. Dual Number方法不能使用, 我们需要构造AST, 从而进行反向求解.  \n",
        "首先, 我们需要构造节点结构, 其中`f`标记函数类型,  $0$为 ${\\rm id}$,  $1$为加,  $2$ 为减, $3$ 为乘...  \n",
        "`sons`为子节点的列表, 长度取决于函数的参数个数.  \n",
        "`value`为该节点的函数值.  \n",
        "`deri`为该节点的导数值.  \n",
        "`id`为该节点的id, 主要用于区分输入."
      ],
      "id": "3044903d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mutable struct OpNode\n",
        "\tf::Int64;\n",
        "\tsons;\n",
        "\tvalue::Float64;\n",
        "\tderi::Float64\n",
        "\tid::Int64;\n",
        "end"
      ],
      "id": "46c6ae2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "接下来, 实现了`OpNode`的默认构造函数, 导数默认置$0$, 等待反向传播时再进行计算. `id`自动加一."
      ],
      "id": "406ff2c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "globalID = 0;\n",
        "function OpNode(f, sons, value)\n",
        "\tglobal globalID;\n",
        "\tOpNode(f, sons, value, 0.0, globalID += 1);\n",
        "end"
      ],
      "id": "640d07a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "之后实现了对几种基本运算的重载:"
      ],
      "id": "9e6a86d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function Base.:+(x::OpNode, y::OpNode)\n",
        "\treturn OpNode(1, [x, y], x.value + y.value);\n",
        "end\n",
        "\n",
        "function Base.:-(x::OpNode, y::OpNode)\n",
        "\treturn OpNode(2, [x, y], x.value - y.value);\n",
        "end\n",
        "\n",
        "function Base.:*(x::OpNode, y::OpNode)\n",
        "\treturn OpNode(3, [x, y], x.value * y.value);\n",
        "end\n",
        "\n",
        "function Base.log(x::OpNode)\n",
        "\treturn OpNode(4, [x], log(x.value));\n",
        "end\n",
        "\n",
        "function Base.sin(x::OpNode)\n",
        "\treturn OpNode(5, [x], sin(x.value));\n",
        "end"
      ],
      "id": "9048a111",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "注意, 至此我们就可以在不修改函数代码的情况下构造出一个AST, 同时完成前向计算各节点的函数值.  \n",
        "现在, 我们可以来实现反向自动微分的核心代码了"
      ],
      "id": "094efaa4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function evaluate(root::OpNode)\n",
        "\tif root.f == 0\n",
        "\t\treturn ;\n",
        "\tend\n",
        "\tif root.f == 1\n",
        "\t\troot.sons[1].deri += root.deri;\n",
        "\t\troot.sons[2].deri += root.deri;\n",
        "\t\tevaluate(root.sons[1]);\n",
        "\t\tevaluate(root.sons[2]);\n",
        "\t\treturn ;\n",
        "\tend\n",
        "\tif root.f == 2\n",
        "\t\troot.sons[1].deri += root.deri;\n",
        "\t\troot.sons[2].deri -= root.deri;\n",
        "\t\tevaluate(root.sons[1]);\n",
        "\t\tevaluate(root.sons[2]);\n",
        "\t\treturn ;\n",
        "\tend\n",
        "\tif root.f == 3\n",
        "\t\troot.sons[1].deri += root.deri*root.sons[2].value;\n",
        "\t\troot.sons[2].deri += root.deri*root.sons[1].value;\n",
        "\t\tevaluate(root.sons[1]);\n",
        "\t\tevaluate(root.sons[2]);\n",
        "\t\treturn ;\n",
        "\tend\n",
        "\tif root.f == 4\n",
        "\t\troot.sons[1].deri += root.deri/root.sons[1].value;\n",
        "\t\tevaluate(root.sons[1]);\n",
        "\t\treturn ;\n",
        "\tend\n",
        "\tif root.f == 5\n",
        "\t\troot.sons[1].deri += root.deri*cos(root.sons[1].value);\n",
        "\t\tevaluate(root.sons[1]);\n",
        "\t\treturn ;\n",
        "\tend\n",
        "end"
      ],
      "id": "7f5734be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "根据每个节点的函数种类, 我们应用链式法则求出其子节点的导数值. 之后递归调用求解其子节点, 直到`sons`为`nothing`,即抵达叶节点为止.  \n",
        "下面这个函数可以从计算完成的树中取出指定`id`的节点的导数值, 用于输出."
      ],
      "id": "422b4741"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function getDeriByID(root::OpNode, id::Int64)\n",
        "\tif root.id == id\n",
        "\t\treturn root.deri;\n",
        "\tend\n",
        "\tif root.sons === nothing\n",
        "\t\treturn ;\n",
        "\tend\n",
        "\tfor i in root.sons\n",
        "\t\tr = getDeriByID(i, id);\n",
        "\t\tif r!==nothing\n",
        "\t\t\treturn r;\n",
        "\t\tend\n",
        "\tend\n",
        "end"
      ],
      "id": "26061f6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "之后, 我们就可以完成反向自动微分的完整逻辑了."
      ],
      "id": "832312b5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "function BackwardAD(f, v::Vector{Float64})::Vector{Float64}\n",
        "\tres = zeros(length(v));\n",
        "\troot = f(map(x->OpNode(0, nothing, x), v));\n",
        "\troot.deri = 1.;\n",
        "\tevaluate(root);\n",
        "\tfor i in 1:length(v)\n",
        "\t\tres[i] = getDeriByID(root, i);\n",
        "\tend\n",
        "\treturn res;\n",
        "end"
      ],
      "id": "5e764616",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们将输入向量`v`转变为一系列叶节点(即`map(x->OpNode(0, nothing, x), v)`语句), 并传入给定函数. 由于我们实现了对各基本操作的重载, 计算该函数时即完成了AST的构建和前向计算各节点函数值的过程, 并返回根节点. 之后, 将根节点的导数值设为1, 并开始反向自动微分. 完成后返回相应的梯度向量即可.  \n",
        "一个相同的例子:"
      ],
      "id": "b772bb89"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "f(x) = log(x[1]) + x[1]*x[2] - sin(x[2]);\n",
        "println(BackwardAD(f, [2., 5.]));"
      ],
      "id": "a6178a28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reference与Tips\n",
        "* 本文中所有图片来自于[arXiv:1502.05767](https://arxiv.org/abs/1502.05767), 如果你想了解更多, 更严格的关于自动微分的内容, 强烈推荐你读一读这篇综述.\n",
        "* 本文中没有考虑多输出函数的情况. 实际上, 标准的Automatic Differentiation程序应当给出Jacobian矩阵, 而不是梯度向量. 相应的, 对于当输出维度很大, 而输入维度相对较小的情况下, 前向自动微分的性能就要优于反向自动微分了.\n",
        "* 关于两种自动微分, 本文中只展现了一个最简单, 最易实现的实现方式. 实际的自动微分有许多细节, 例如, 对AST进行图优化, 基于元编程的Differiatial Rule生成等等. 如果你想深入学习, 你应当读一读大项目中的AD实现. 作为推荐, Julia目前实现了一套相当好的可微分编程环境, 你可以读一读[FowardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)以及[ReverseDiff.jl](https://github.com/JuliaDiff/ReverseDiff.jl)的代码."
      ],
      "id": "8196e49f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.8",
      "language": "julia",
      "display_name": "Julia 1.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}